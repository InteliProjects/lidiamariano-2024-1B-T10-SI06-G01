{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HvAqwwa0YZj"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp3uT5az0r4K"
      },
      "source": [
        "All libraries that are used throughout the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oraxNnjo0WEM",
        "outputId": "1c08c4cd-957c-4923-b79d-e1cc53cc13ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 821, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 821, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 2348, in parseImpl\n",
            "    raise ParseException(instring, loc, self.errmsg, self)\n",
            "pip._vendor.pyparsing.exceptions.ParseException: Expected '(', found ';'  (at char 19), (line:1, col:20)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 207, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 383, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.10/traceback.py\", line 306, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.10/linecache.py\", line 137, in updatecache\n",
            "    lines = fp.readlines()\n",
            "  File \"/usr/lib/python3.10/codecs.py\", line 319, in decode\n",
            "    def decode(self, input, final=False):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 825, in _parseNoCache\n",
            "    ret_tokens = ParseResults(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/results.py\", line 181, in __init__\n",
            "    self[name]._name = name\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/results.py\", line 196, in __getitem__\n",
            "    return self._tokdict[i][-1][0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/results.py\", line 17, in __getitem__\n",
            "    def __getitem__(self, i):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1740, in isEnabledFor\n",
            "    level >= self.getEffectiveLevel()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1718, in getEffectiveLevel\n",
            "    while logger:\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy\n",
        "%pip install matplotlib\n",
        "%pip install scikit-learn\n",
        "%pip install seaborn\n",
        "%pip install keras\n",
        "%pip install tensorflow\n",
        "%pip install plotly_express\n",
        "%pip install nltk\n",
        "%pip install gensim\n",
        "%pip install gdown\n",
        "%pip install fasttext\n",
        "%pip install imbalanced-learn\n",
        "%pip install nltk pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv00W2UUOlYr",
        "outputId": "abfd370a-5b7c-437c-c401-a3e9df38d4c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m913.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4227137 sha256=15a3b2c445591b9b99f984744c2f377761855f2269189bbef149bb5a2c47b1d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.12.0\n"
          ]
        }
      ],
      "source": [
        "%pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i6u5JGgs0xtE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout\n",
        "from gensim.models import KeyedVectors\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, fbeta_score, classification_report\n",
        "from keras.utils import to_categorical\n",
        "import gdown\n",
        "from gensim.models import FastText\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, learning_curve\n",
        "import string\n",
        "import re\n",
        "import fasttext\n",
        "from sklearn import svm\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from sklearn.utils import resample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkvK8AEi1GSi"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EErvzK1a1JCD"
      },
      "source": [
        "Word2Vec is a machine learning model that transforms words into numerical vectors, capturing their contexts and semantic meanings. Trained on large text datasets, it utilizes techniques such as Continuous Bag of Words (CBOW) and Skip-gram to predict words based on their contexts or vice versa. Words that occur in similar contexts end up with vectors that are close together in a high-dimensional space. These vectors, known as embeddings, enable the capture of semantic relationships between words and are used in various natural language processing (NLP) tasks such as text classification and machine translation. For more information about the model visit documentation [session 9.3 Word2Vec](https://github.com/Inteli-College/2024-1B-T10-SI06-G01/blob/main/documentos/documentation.md#93-word2vec)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8e9lMF31OJG"
      },
      "source": [
        "In this section, we will be demonstrating Word2Vec in a way that does not use pre-trained models, meaning that the authorship of the results is entirely by the group."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqz80dke1RT7"
      },
      "source": [
        "Data reading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "QbE6pGqB1PdM",
        "outputId": "6ac8edfd-7bab-4e71-dccf-c99875f38bdc"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/classification-labeled.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-88aac962d1d1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/classification-labeled.csv'"
          ]
        }
      ],
      "source": [
        "path = 'data/classification-labeled.csv'\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(path, encoding='ISO-8859-1', delimiter=';')\n",
        "except UnicodeDecodeError:\n",
        "    try:\n",
        "        data = pd.read_csv(path, encoding='windows-1252')\n",
        "    except UnicodeDecodeError:\n",
        "        data = pd.read_csv(path, encoding='utf-16')\n",
        "\n",
        "\n",
        "sentences = data['comment'].apply(simple_preprocess).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c6vveAr1e5-"
      },
      "source": [
        "Word2Vec training:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dxkLD4o1h-X"
      },
      "source": [
        "Here we can define if we use either CBOW or skip-gram, if it's desirable to use skip-grap, we add the parameter (sg=1), and if CBOW is considered as the best option, the parameter isn't necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRt_om1t1c6r"
      },
      "outputs": [],
      "source": [
        "word2vec_model = Word2Vec(sentences=sentences, vector_size=600, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKSatTY11mR4"
      },
      "source": [
        "Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dMQJcoZ16nW"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in sequences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "data_padded = pad_sequences(sequences, maxlen=max_sequence_length)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyK0uTI_19vt"
      },
      "source": [
        "Creation of the vectors for each sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-CL9U-H1-E6"
      },
      "outputs": [],
      "source": [
        "def get_sum_vector(sentence, model, vector_size):\n",
        "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "    if vectors:\n",
        "        return np.sum(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "vector_size = 600\n",
        "X = np.array([get_sum_vector(sentence, word2vec_model, vector_size) for sentence in sentences])\n",
        "y = data['sentiment'].values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCXmifi2AP2"
      },
      "source": [
        "Data normalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SS53pX21__q"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VSr8MaM2C3R"
      },
      "source": [
        "Model Fit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd0qsYHK2D4t"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "classifier = GaussianNB(var_smoothing=1e-09)\n",
        "classifier.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hejaX0V2NUR"
      },
      "source": [
        "Model testeing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoEcYOew2K2N"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gkAocTt2N3L"
      },
      "source": [
        "Dataframe and csv creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USV5hf562PJi"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(X_scaled)\n",
        "df['sentiment'] = y\n",
        "\n",
        "df.to_csv('word2vec_features.csv', index=False)\n",
        "path2 = '/content/word2vec_features.csv'\n",
        "df_w2v = pd.read_csv(path2)\n",
        "df_w2v.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N1ZlU6k3vH1"
      },
      "source": [
        "### Naive Bayes Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYAKJ0jF33Ew"
      },
      "source": [
        "#### Why Naive Bayes?\n",
        "\n",
        "&emsp;&emsp;In this notebook, we implement different variants of the Naive Bayes model for the task of sentiment classification of Uber customer reviews. This approach uses pre-trained word embeddings (Word2Vec) to convert texts into vector representations, which are then used to train the models.\n",
        "\n",
        "&emsp;&emsp;Naive Bayes is often chosen as a baseline model for text classification tasks due to several reasons:\n",
        "1. Simplicity and Efficiency: Naive Bayes is one of the simplest and fastest machine learning algorithms. Its implementation is straightforward, and it requires fewer computational resources, making it suitable for large volumes of data.\n",
        "2. Few Parameters to Tune: Naive Bayes has few parameters to tune, which simplifies its use and reduces the need for extensive hyperparameter optimization. This is particularly useful in an initial scenario where the focus is on obtaining results quickly for a preliminary evaluation.\n",
        "\n",
        "#### Limitations of Naive Bayes:\n",
        "\n",
        "&emsp;&emsp;Although Naive Bayes is an excellent starting point, it has its limitations:\n",
        "1. Independence Assumption: Naive Bayes assumes that all features are independent of each other. In the context of text, this means it considers each word in a document independently, ignoring the dependency between words that can be crucial for understanding the context.\n",
        "2. Performance on Imbalanced Data: Naive Bayes can be sensitive to imbalanced data. If one class is significantly larger than others, the model may perform biasedly in favor of the majority class.\n",
        "3. Doesn't Capture Complex Relationships: Due to its simplicity, Naive Bayes cannot capture complex relationships between features. More advanced models, like neural networks, can learn these relationships and generally outperform Naive Bayes on complex tasks.\n",
        "4. Scalability with High Dimensionality: Although it is efficient with many features, Naive Bayes can face difficulties when the data dimensionality is extremely high, especially if the data is not well-prepared or if there is no dimensionality reduction.\n",
        "\n",
        "#### Implementation\n",
        "&emsp;&emsp;In this notebook, we explore three variants of Naive Bayes:\n",
        "\n",
        "1. Gaussian Naive Bayes: Suitable for continuous data. We normalized the data before applying this model.\n",
        "2. Multinomial Naive Bayes: Suitable for discrete data, such as word counts. We binarized the data before applying this model.\n",
        "3. Bernoulli Naive Bayes: Suitable for binary data. We binarized the data before applying this model.\n",
        "\n",
        "#### Results\n",
        "\n",
        "&emsp;&emsp;We evaluated each model using performance metrics such as F1 Score, Recall, Precision, and Accuracy, with a primary focus on F1 Score, Recall, and Precision. The emphasis on these metrics is due to the project's critical need to avoid missing true negative values. Additionally, we plotted confusion matrices to visualize the performance in terms of correct and incorrect classifications.\n",
        "&emsp;&emsp;Naive Bayes provides a solid foundation for the sentiment classification task, but its limitations indicate the need to explore more complex models to improve performance. This notebook serves as a starting point to understand the challenges and guide future developments towards more sophisticated models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCY-N4y24MTU"
      },
      "source": [
        "Loading the Dataset and Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LKPGIJu3z3g"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "#path2 = '../src/word2vec_features.csv'\n",
        "path2 = '/content/word2vec_features.csv'\n",
        "df_w2v = pd.read_csv(path2)\n",
        "X = df_w2v.drop('sentiment', axis=1)\n",
        "y = df_w2v['sentiment']\n",
        "\n",
        "# Splitting the Data into Training and Validation Sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data normalization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdgIzi44K-a"
      },
      "source": [
        "Training and Evaluation of Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziFt6BdM4LGW"
      },
      "outputs": [],
      "source": [
        "# Training the Gaussian Naive Bayes model with adjusted var_smoothing\n",
        "classifier_gnb = GaussianNB(var_smoothing=1e-09)\n",
        "classifier_gnb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Prediction on the validation data\n",
        "y_pred_gnb = classifier_gnb.predict(X_val_scaled)\n",
        "\n",
        "# Calculation of metrics\n",
        "f1_gnb = f1_score(y_val, y_pred_gnb, average='weighted')\n",
        "recall_gnb = recall_score(y_val, y_pred_gnb, average='weighted')\n",
        "precision_gnb = precision_score(y_val, y_pred_gnb, average='weighted')\n",
        "accuracy_gnb = accuracy_score(y_val, y_pred_gnb)\n",
        "\n",
        "# Displaying the metrics\n",
        "print(\"Gaussian Naive Bayes:\")\n",
        "print(f\"F1 Score: {f1_gnb:.4f}\")\n",
        "print(f\"Recall: {recall_gnb:.4f}\")\n",
        "print(f\"Precision: {precision_gnb:.4f}\")\n",
        "print(f\"Accuracy: {accuracy_gnb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75AnORa04O70"
      },
      "source": [
        "Training and Evaluation of Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzjyRkwm4QEr"
      },
      "outputs": [],
      "source": [
        "# Binarizing the selected data\n",
        "binarizer = Binarizer()\n",
        "X_train_binarized = binarizer.fit_transform(X_train_scaled)\n",
        "X_val_binarized = binarizer.transform(X_val_scaled)\n",
        "\n",
        "# Training the Multinomial Naive Bayes model\n",
        "classifier_mnb = MultinomialNB()\n",
        "classifier_mnb.fit(X_train_binarized, y_train)\n",
        "\n",
        "# Prediction on the validation data\n",
        "y_pred_mnb = classifier_mnb.predict(X_val_binarized)\n",
        "\n",
        "# Calculation of metrics\n",
        "f1_mnb = f1_score(y_val, y_pred_mnb, average='weighted')\n",
        "recall_mnb = recall_score(y_val, y_pred_mnb, average='weighted')\n",
        "precision_mnb = precision_score(y_val, y_pred_mnb, average='weighted')\n",
        "accuracy_mnb = accuracy_score(y_val, y_pred_mnb)\n",
        "\n",
        "# Displaying the metrics\n",
        "print(\"Multinomial Naive Bayes:\")\n",
        "print(f\"F1 Score: {f1_mnb:.4f}\")\n",
        "print(f\"Recall: {recall_mnb:.4f}\")\n",
        "print(f\"Precision: {precision_mnb:.4f}\")\n",
        "print(f\"Accuracy: {accuracy_mnb:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKxRdl5i4Rba"
      },
      "source": [
        "Training and Evaluation of Bernoulli Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4UMyeEe4Sc5"
      },
      "outputs": [],
      "source": [
        "# Training the Bernoulli Naive Bayes model\n",
        "classifier_bnb = BernoulliNB()\n",
        "classifier_bnb.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Prediction on the validation data\n",
        "y_pred_bnb = classifier_bnb.predict(X_val_scaled)\n",
        "\n",
        "# Calculation of metrics\n",
        "f1_bnb = f1_score(y_val, y_pred_bnb, average='weighted')\n",
        "recall_bnb = recall_score(y_val, y_pred_bnb, average='weighted')\n",
        "precision_bnb = precision_score(y_val, y_pred_bnb, average='weighted')\n",
        "accuracy_bnb = accuracy_score(y_val, y_pred_bnb)\n",
        "\n",
        "# Displaying the metrics\n",
        "print(\"Bernoulli Naive Bayes:\")\n",
        "print(f\"F1 Score: {f1_bnb:.4f}\")\n",
        "print(f\"Recall: {recall_bnb:.4f}\")\n",
        "print(f\"Precision: {precision_bnb:.4f}\")\n",
        "print(f\"Accuracy: {accuracy_bnb:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXmW0e_W4T14"
      },
      "outputs": [],
      "source": [
        "# Function to plot the confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plotting confusion matrix for Gaussian Naive Bayes\n",
        "plot_confusion_matrix(y_val, y_pred_gnb, title='Confusion Matrix - Gaussian Naive Bayes')\n",
        "\n",
        "# Plotting confusion matrix for Multinomial Naive Bayes\n",
        "plot_confusion_matrix(y_val, y_pred_mnb, title='Confusion Matrix - Multinomial Naive Bayes')\n",
        "\n",
        "# Plotting confusion matrix for Bernoulli Naive Bayes\n",
        "plot_confusion_matrix(y_val, y_pred_bnb, title='Confusion Matrix - Bernoulli Naive Bayes')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utMhu9L-4bSH"
      },
      "source": [
        "Metrics\n",
        "\n",
        "Given the project's emphasis on correctly identifying negative sentiments, recall is the most critical metric. A higher recall indicates the model is effectively identifying most of the negative instances, reducing the chances of overlooking critical feedback.\n",
        "\n",
        "**Gaussian Naive Bayes:**\n",
        "- **Highest Precision (0.7372):** Indicates the model is good at predicting negative tweets correctly when it predicts them.\n",
        "- **Highest Recall (0.5781):** Among the three models, this one captures the most actual negative tweets.\n",
        "- **Highest F1 Score (0.6343):** Balances precision and recall, suggesting overall better performance.\n",
        "\n",
        "**Multinomial Naive Bayes:**\n",
        "- **Recall (0.5694):** Slightly lower than Gaussian Naive Bayes but still competitive.\n",
        "- **Precision (0.7002):** Slightly lower than Gaussian Naive Bayes but better than Bernoulli Naive Bayes.\n",
        "- **F1 Score (0.6197):** Lower than Gaussian Naive Bayes, indicating slightly less balanced performance.\n",
        "\n",
        "**Bernoulli Naive Bayes:**\n",
        "- **Recall (0.5590):** Lowest recall, indicating it misses more negative tweets compared to the other models.\n",
        "- **Precision (0.7118):** Better than Multinomial Naive Bayes but not as good as Gaussian Naive Bayes.\n",
        "- **F1 Score (0.6134):** Lowest overall, indicating less effective balance between precision and recall.\n",
        "\n",
        "Given the need to prioritize the identification of true negatives (to minimize false negatives), **Gaussian Naive Bayes** is the preferred model. It achieves the highest recall, which is critical for capturing as many negative tweets as possible. Additionally, it has the highest F1 score, suggesting a good balance between precision and recall, making it the best model amoung the others for this task. For more information about the metrics visit [12.3 Word2Vec](https://github.com/Inteli-College/2024-1B-T10-SI06-G01/blob/main/documentos/documentation.md#123-Word2Vec). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW3r2Np55TI2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85waSOcQ7ZWX"
      },
      "source": [
        "### Embeding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YRO9xIV_UY2"
      },
      "source": [
        "Embedding layer is a type of neural network layer commonly used in natural language processing to transform sparse, high-dimensional categorical data (like words) into dense, lower-dimensional continuous vectors. Each unique word in a vocabulary is mapped to a vector of real numbers which are learned as the model trains. These vectors capture semantic relationships between words, such that words with similar meanings are located close to each other in the vector space. This transformation facilitates more efficient processing and enables the model to perform better on tasks like classification, translation, and sentiment analysis by understanding the context and nuances of language better.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qb569I2_y2j"
      },
      "source": [
        "one-hot-ecoding of labels and applying SMOTE for class balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2riVOZR_vAl"
      },
      "outputs": [],
      "source": [
        "labels = data['sentiment'].values + 1\n",
        "labels_one_hot = to_categorical(labels, num_classes=3)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(data_padded, labels_one_hot)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_smote, y_train_smote, test_size=0.4, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrCnieGT_2ud"
      },
      "source": [
        "Creating the embedding matrix and model: The parameters of the model are adjusted for a multiclass classification model. Some parameters that are worth mentioning are:\n",
        "\n",
        "- The big Dropout number, that in most cases isn't ideal, but as we are trying to avoid overfitting at all cost it is necessary.\n",
        "- The softmax activation function is also ideal for multiclass classification problems.\n",
        "- It is worth mentioning that as we are not working with a binary classification problem, it's necessary to use multiclass loss functions, such as the categorical_crossentropy function.\n",
        "- The model is classified as trainable model, which lets the allows the model to learn from itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1GzcX8K_1-X"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 600\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=True))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA-n4FLaAEZt"
      },
      "source": [
        "Compiles the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O4E8D5qAAvp"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYsGBkvxAHHH"
      },
      "source": [
        "Generating the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBrvMnspAJDr"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_val)\n",
        "\n",
        "\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_val, axis=1)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqrXOYdRALVK"
      },
      "source": [
        "As we can see, the model is very effective in identifying negative coments, succeding in >93% of cases (372 of 396). The cons of the embeding layer model are observed at identifying the positive comments which are beeing classified as neutral, which isn't a drawback if using the model commercialy, but affects the accuracy directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvuugAO1SPnO"
      },
      "source": [
        "### Word2Vec with pre-trained dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvGU_YOhSWSC"
      },
      "source": [
        "**Downloading pre-trained model**\n",
        "\n",
        "This section mounts Google Drive to access files and downloads the pre-trained Word2Vec model and the dataset using the file IDs provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ADyOCoUSTeV"
      },
      "outputs": [],
      "source": [
        "# Permission to access Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv_ojOVmSeJJ"
      },
      "source": [
        " **Importing pre-trained model**\n",
        "\n",
        "The pre-trained Word2Vec model is loaded from the downloaded binary file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIvVVZWiScDE"
      },
      "outputs": [],
      "source": [
        "file_id = \"1nJ-Xl3rJ5lB_BH0yJ1YIRC4_ExNCU5L1\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"GoogleNews-vectors-negative300.bin\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXvGztzMSk1P"
      },
      "outputs": [],
      "source": [
        "pre_processed_model = 'GoogleNews-vectors-negative300.bin'\n",
        "word_vectors = KeyedVectors.load_word2vec_format(pre_processed_model, binary=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6BIi-vTSjsV"
      },
      "source": [
        "**Importing Uber dataset**\n",
        "\n",
        "The dataset is loaded from the downloaded CSV file using pandas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocWnPKG1S3By"
      },
      "outputs": [],
      "source": [
        "file_id = \"1D7MY7QKnOXMPWx6hqd0ASKrJRPjE3u_\"\n",
        "url = f\"https://drive.google.com/file/d/{file_id}-/view?usp=drive_link\"\n",
        "output = \"classification-labeled.csv\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfVH7Px6S6e3"
      },
      "outputs": [],
      "source": [
        "#dataset = pd.read_csv(\"classification-labeled.csv\", sep = \";\")\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/dbs/classification-labeled.csv\", encoding='ISO-8859-1', delimiter=';')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVNjecW1S69E"
      },
      "source": [
        "**Testing a random vector**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBoE6ztJS-mF"
      },
      "outputs": [],
      "source": [
        "v_hate = word_vectors[\"hate\"]\n",
        "\n",
        "v_hate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc7peIssTBCb"
      },
      "source": [
        "**Pre-processing Uber dataset**\n",
        "\n",
        "Preprocesses comments in a dataset by removing stopwords, URLs, punctuation, and numbers.\n",
        "\n",
        "  This function takes a dataset and a column as input. It first tokenizes the comments in the specified column using a WhitespaceTokenizer. Then, it removes stopwords (either in English or Portuguese), URLs, punctuation, and numbers from each comment. The cleaned comments are returned as a list.\n",
        "\n",
        "  Parameters:\n",
        "  dataset (pandas.DataFrame): The dataset containing the comments.\n",
        "  column (str): The name of the column containing the comments.\n",
        "\n",
        "  Returns:\n",
        "  list: A list of processed comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnS7X-mZTHAG"
      },
      "outputs": [],
      "source": [
        "# Função de pré-processamento\n",
        "def preprocess_comments(dataset, column):\n",
        "    nltk.download('stopwords')\n",
        "    stopwords = set(sw.words('english'))\n",
        "    token_space = WhitespaceTokenizer()\n",
        "\n",
        "    phase_processing = list()\n",
        "    for comment in dataset[column]:\n",
        "        word_text = token_space.tokenize(comment)\n",
        "        new_comment = [w.lower() for w in word_text if not w.lower() in stopwords] # lowercase\n",
        "        new_comment = [re.sub(r'http\\S+|www.\\S+', '', word) for word in new_comment] # remove HTML marks\n",
        "        new_comment = [re.sub(r'[^\\w\\s]', '', word) for word in new_comment]  # remove punctuation\n",
        "        new_comment = [re.sub(r'\\d+', '', word) for word in new_comment]  # remove numbers\n",
        "        phase_processing.append(' '.join(new_comment))\n",
        "\n",
        "    return phase_processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZWpFGcLTRIR"
      },
      "outputs": [],
      "source": [
        "# Calling a function preprocess_comments to preprocess a dataset of comments\n",
        "preprocessing = preprocess_comments(dataset, 'comment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqjXLDaNTSQ0"
      },
      "outputs": [],
      "source": [
        "# Assigning the result of the preprocessing operation to a new column named 'preprocessing_data'\n",
        "dataset['preprocessing_data'] = preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MjZyu6jTTU4"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV73a-H3TUv1"
      },
      "outputs": [],
      "source": [
        "# See the content of the newly created column\n",
        "dataset['preprocessing_data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Dg-WdtCTV2E"
      },
      "source": [
        "**Generating a vector representation**\n",
        "\n",
        "Using the *get_sum_vector* function to generate vector representations for each sentence in the dataset's 'preprocessing_data' column and then creating a NumPy array X containing these vectors. Additionally, creating a NumPy array y containing the corresponding sentiment labels from the 'sentiment' column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZm0X5TvTZD0"
      },
      "outputs": [],
      "source": [
        "def get_sum_vector(sentence, model, vector_size):\n",
        "    vectors = [model[word] for word in sentence.split() if word in model]\n",
        "    return np.sum(vectors, axis=0) if vectors else np.zeros(vector_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nsPFzhoTa4M"
      },
      "outputs": [],
      "source": [
        "vector_size = 300\n",
        "\n",
        "X = np.array([get_sum_vector(comment, word_vectors, vector_size) for comment in dataset['preprocessing_data']])\n",
        "y = dataset['sentiment'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UbZT3sZTb_z"
      },
      "source": [
        "**Verifying class balancing**\n",
        "\n",
        "Address class imbalance to prevent the model from favoring the majority class. There have been used Random OverSampler to balance the distribution correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKu_eTZdTeMs"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=y)\n",
        "plt.title('Class distribution before balancing')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YA660u6TgKC"
      },
      "outputs": [],
      "source": [
        "# Using Random OverSampler to balance class\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7sxYFhqThIK"
      },
      "outputs": [],
      "source": [
        "# Verifying new balance\n",
        "sns.countplot(x=y_resampled)\n",
        "plt.title('Class distribution after balancing')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKOvrzQOTj7k"
      },
      "source": [
        "**Gaussian Naive Bayes Classifier**\n",
        "\n",
        "Training Naive Bayes classifier to identify the Word2Vec model performance. For that, the balanced data was splitted into train and test. The Gaussian Naive Bayes was choosed for the classification, as it has the best performance for this kind of data. In the end, it was calculated the scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwZk3NqPTnqo"
      },
      "outputs": [],
      "source": [
        "# Dividing data sample between train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4xsavDgToqL"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXDceYBoTqXS"
      },
      "outputs": [],
      "source": [
        "# Prevision on test sample\n",
        "y_pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw_fHN7NTr-d"
      },
      "outputs": [],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5rV5b79Ts0C"
      },
      "outputs": [],
      "source": [
        "# Plot confusion Matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
        "plt.xlabel('Predict')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlgIHTgcTupN"
      },
      "source": [
        "**Calculating precision metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32wRrghyTuR0"
      },
      "outputs": [],
      "source": [
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVF0QXlkTzI_"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more information about the metrics visit [12.3 Word2Vec](https://github.com/Inteli-College/2024-1B-T10-SI06-G01/blob/main/documentos/documentation.md#123-Word2Vec). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_KhnjEAJxKe"
      },
      "source": [
        "## FastText\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7GgkRnsR6kL"
      },
      "source": [
        "FastText is a word representation model and machine learning technique developed by Facebook. It is an extension of the Word2Vec model, but instead of representing words as single vectors, FastText represents each word as a sum of n-gram vectors of characters. This allows FastText to handle rare words better and with less data, as even unknown words can be represented as n-gram combinations of known characters. For more information about the model visit documentation  [session 9.5 FastText](https://github.com/Inteli-College/2024-1B-T10-SI06-G01/blob/main/documentos/documentation.md#95-fasttext).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgJoKk7sLILY"
      },
      "source": [
        "**Text Preprocessing Function:**\n",
        "This function preprocesses the text data. It involves several steps:\n",
        "\n",
        "Converting text to lowercase.\n",
        "Removing punctuation.\n",
        "Removing numbers.\n",
        "Tokenizing text into words.\n",
        "Removing stop words.\n",
        "The function takes a text string as input and returns the preprocessed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGbLgi3RLHHE"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbfTywhHLNYt"
      },
      "outputs": [],
      "source": [
        "# Text Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text:\n",
        "        - Conversion to lowercase\n",
        "        - Removal of punctuation\n",
        "        - Removal of numbers\n",
        "        - Tokenization\n",
        "        - Removal of stop words\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "        str: Preprocessed text.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a string\n",
        "    text = ' '.join(words)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6bxNXlBLQ7A"
      },
      "source": [
        "This function converts text to a vector representation using the FastText model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rFU8s8ELP3B"
      },
      "outputs": [],
      "source": [
        "def text_to_vector(text):\n",
        "    \"\"\"\n",
        "    Converts text to vector using FastText.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be converted to vector.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Vector representing the text.\n",
        "    \"\"\"\n",
        "    # Text to vector conversion using FastText\n",
        "    return model.get_sentence_vector(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YOmsqInLS76"
      },
      "source": [
        "**Load the Dataset and Preprocess Texts:** In this part, the dataset is loaded from a CSV file into a Pandas DataFrame. Text and label columns are extracted from the dataset. Texts are preprocessed using the preprocess_text() function defined earlier. Labels are converted to the FastText format with a prefix \"label\".\n",
        "\n",
        "**Create Temporary Training File in FastText Format:** A temporary training file in FastText format is created. It iterates over preprocessed texts and corresponding labels, and writes them to the file in the format required by FastText. Each line in the file contains a label followed by the preprocessed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c71wT7TLTJy"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = pd.read_csv(\"classification-labeled.csv\", sep=\";\")\n",
        "\n",
        "texts = dataset['comment'].tolist()\n",
        "labels = dataset['sentiment'].tolist()\n",
        "\n",
        "texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "labels = [f'__label__{label}' for label in labels]\n",
        "\n",
        "train_data_path = 'train_data.txt'\n",
        "with open(train_data_path, 'w', encoding='utf-8') as f:\n",
        "    for text, label in zip(texts, labels):\n",
        "        f.write(f'{label} {text}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv91YsbCLVZM"
      },
      "source": [
        "**Split Data into Train and Test Sets:**\n",
        "The dataset is split into training and testing sets using the train_test_split function from scikit-learn. 85% of the data is used for training and 15% for testing. This step is crucial for evaluating the performance of the trained model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9xZh4p4LWdl"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.15, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHNy8zZwLXr0"
      },
      "source": [
        "**Train the FastText Model:**\n",
        "This section initializes and trains the FastText model. Parameters like vector size, window size, minimum count, etc., are set during model initialization. The model vocabulary is built using the training data. Then, the model is trained on the training data using the train() method, passing the file path to the training data and other required parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mym7GZ0VLY72"
      },
      "outputs": [],
      "source": [
        "model = FastText(vector_size=100, window=5, min_count=5, workers=4, sg=1, epochs=10)\n",
        "model.build_vocab(corpus_file=train_data_path)\n",
        "total_words = sum(1 for line in open(train_data_path, 'r'))\n",
        "model.train(corpus_file=train_data_path, total_examples=len(texts), total_words=total_words, epochs=model.epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpWWBGSqLa19"
      },
      "source": [
        "**Generate Embeddings for Test Sentences:**\n",
        "After training the FastText model, embeddings are generated for the test sentences. Each test sentence is tokenized into words, and the word vectors are averaged to obtain a sentence vector. These sentence vectors are used as input features for the classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbsgjSPoLb8E"
      },
      "outputs": [],
      "source": [
        "X_test_vectors = [model.wv[sentence.split()].mean(axis=0) for sentence in X_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWD4UA7ZLeNP"
      },
      "source": [
        "**Initialize and Train the Naive Bayes Classifier:**\n",
        "A Gaussian Naive Bayes classifier is initialized. The sentence vectors obtained from the FastText model for the training set are used to fit the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdCAb1_TLebK"
      },
      "outputs": [],
      "source": [
        "classifier = GaussianNB()\n",
        "X_train_vectors = [model.wv[sentence.split()].mean(axis=0) for sentence in X_train]\n",
        "classifier.fit(X_train_vectors, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FgPHxs2LgJO"
      },
      "source": [
        "**Make Predictions on the Test Set and Evaluate the Model:**\n",
        "Predictions are made on the test set using the trained classifier. Various evaluation metrics such as accuracy, precision, recall, and F1-score are calculated using scikit-learn's functions. These metrics provide insights into the performance of the model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_3OuL1iLhXY"
      },
      "outputs": [],
      "source": [
        "y_pred = classifier.predict(X_test_vectors)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1_score = fbeta_score(y_test, y_pred, beta=2, average='weighted')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHLoNBQiLjGz"
      },
      "source": [
        "**Plot Confusion Matrix:**\n",
        "Finally, a confusion matrix is generated using scikit-learn's confusion_matrix function. The confusion matrix provides a visual representation of the model's performance by comparing predicted labels with true labels. It helps in identifying the number of correct and incorrect predictions for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0yVWQHvLllD"
      },
      "outputs": [],
      "source": [
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4HkRrA_LnJa"
      },
      "source": [
        "#### **Results**\n",
        "\n",
        "FastText is a text classification technique developed by Facebook AI Research (FAIR). It offers an efficient approach to representing words and texts in a continuous vector space, capturing both semantic and structural information.\n",
        "\n",
        "Results:</br>\n",
        "\n",
        "Accuracy: 56.94%</br>\n",
        "Precision: 75.60%</br>\n",
        "Recall: 56.94%</br>\n",
        "F1-score: 57.81%</br>\n",
        "\n",
        "The results indicate a moderate capability for text classification. However, a more in-depth analysis and cross-validation are recommended to confirm the model's robustness and to understand its limitations. For more information about the metrics visit [12.7 FastText](https://github.com/Inteli-College/2024-1B-T10-SI06-G01/blob/main/documentos/documentation.md#127-fasttext). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WugfP9RpLuYI"
      },
      "source": [
        "### FastText + SVM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9ZDVY59LwWc"
      },
      "source": [
        "In this approach, we combine the power of the FastText model for word representation with the effectiveness of the SVM classifier to perform sentiment classification on Uber comments. FastText allows capturing rich semantic information, while SVM is adept at finding patterns in high-dimensional spaces. Together, these models can process comments, extract their semantic features, and classify their sentiments accurately. Training is performed by tuning the hyperparameters of FastText and SVM, followed by evaluating the model's performance using metrics such as precision, recall, and F1-score. This combination provides an effective way to understand and classify the sentiments expressed by users in Uber comments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SSNXEzYLya9"
      },
      "source": [
        "In this section, we delve into advanced techniques to enhance the performance of our text classification model using FastText combined with Support Vector Machines (SVM). We explore the utilization of Hierarchical Softmax (HS) in FastText, the incorporation of word bigrams for improved feature representation, and the significance of performing text preprocessing within the FastText+SVM pipeline.\n",
        "\n",
        "1. Hierarchical Softmax:\n",
        "Hierarchical Softmax is a specialized loss function utilized in FastText for more efficient training and prediction processes. Unlike traditional softmax, which computes probabilities for each label independently, Hierarchical Softmax organizes labels into a hierarchical structure, significantly reducing the computational complexity by efficiently traversing the label hierarchy during training and prediction. This enables faster training and inference, particularly beneficial when dealing with large datasets and numerous labels.\n",
        "\n",
        "2. Utilizing Word Bigrams:\n",
        "Incorporating word bigrams, sequences of two adjacent words, allows our model to capture more contextual information and linguistic nuances present in the text data. By considering word pairs instead of individual words, the model gains a richer understanding of language patterns and dependencies, leading to enhanced classification accuracy, especially in tasks where the order of words is crucial, such as sentiment analysis and document categorization.\n",
        "\n",
        "3. Preprocessing Within FastText+SVM Section:\n",
        "While the notebook initially includes text preprocessing steps, we opt to integrate the preprocessing directly into the FastText+SVM pipeline. By doing so, we ensure consistency in the preprocessing techniques applied across the entire workflow. Additionally, embedding preprocessing within the training pipeline facilitates seamless model deployment and reduces the complexity of managing multiple preprocessing steps independently. This streamlined approach improves code readability, maintainability, and scalability of the classification system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArNDRIgFL2y5"
      },
      "source": [
        "This function preprocesses the input text by converting it to lowercase, removing punctuation and numbers, tokenizing the text, and removing stop words using NLTK.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOnB47-yLo3m"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text:\n",
        "        - Conversion to lowercase\n",
        "        - Removal of punctuation\n",
        "        - Removal of numbers\n",
        "        - Tokenization\n",
        "        - Removal of stop words\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "        str: Preprocessed text.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a string\n",
        "    text = ' '.join(words)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIQqjs1mL5to"
      },
      "source": [
        "This function converts text to a vector representation using the FastText model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH5VdnGBL34c"
      },
      "outputs": [],
      "source": [
        "def text_to_vector(text):\n",
        "    \"\"\"\n",
        "    Converts text to vector using FastText.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be converted to vector.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Vector representing the text.\n",
        "    \"\"\"\n",
        "    # Text to vector conversion using FastText\n",
        "    return model.get_sentence_vector(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e81N4dM2L9Jr"
      },
      "source": [
        "This section loads the dataset from a CSV file, preprocesses the text data using the previously defined preprocess_text function, and formats the data into the format expected by FastText. The preprocessed data is then written to a temporary file for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Jv7PSzL6kK"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset = pd.read_csv(\"classification-labeled.csv\", sep=\";\")\n",
        "\n",
        "\n",
        "texts = dataset['comment'].tolist()\n",
        "labels = dataset['sentiment'].tolist()\n",
        "\n",
        "texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "labels = [f'__label__{label}' for label in labels]\n",
        "\n",
        "train_data_path = 'train_data.txt'\n",
        "with open(train_data_path, 'w', encoding='utf-8') as f:\n",
        "    for text, label in zip(texts, labels):\n",
        "        f.write(f'{label} {text}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs24UDgkMFx_"
      },
      "source": [
        "This part trains the FastText model using the hierarchical softmax (HS) loss function and saves the trained model to a binary file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDq1pc7YMF5n"
      },
      "outputs": [],
      "source": [
        "model = fasttext.train_supervised(input=train_data_path, lr=1.0, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='hs')\n",
        "model.save_model('fasttext_model_hs.bin')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9-R01ycMJfA"
      },
      "source": [
        "Here, the text data is converted into feature vectors using the FastText model, and the corresponding labels are prepared for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umLnsv3xMHIB"
      },
      "outputs": [],
      "source": [
        "X = [text_to_vector(text) for text in texts]\n",
        "y = [int(label.replace('__label__', '')) for label in labels]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLayURLPMMyK"
      },
      "source": [
        "This section splits the data into training and testing sets, and then applies Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance by oversampling the minority class in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXduR3_XMKQs"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcUHlw0oMOO4"
      },
      "source": [
        "This part involves training a Support Vector Machine (SVM) classifier using the linear kernel on the resampled training data. It then evaluates the classifier's performance on the test set using various evaluation metrics such as accuracy, precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3a-5l0AMPE1"
      },
      "outputs": [],
      "source": [
        "svm_classifier = svm.SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1_score = fbeta_score(y_test, y_pred, beta=2, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1-score: {f1_score}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYw0dbl3MQx5"
      },
      "source": [
        "**Confusion Matrix**\n",
        "\n",
        "The confusion matrix provides a visual representation of the model's performance by showing the counts of true positive, true negative, false positive, and false negative predictions for each class label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hVWxAiTMScJ"
      },
      "outputs": [],
      "source": [
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(set(labels)), yticklabels=sorted(set(labels)))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKnh4S46MVWH"
      },
      "source": [
        "#### **Results**\n",
        "\n",
        "The FastText + SVM model exhibited promising results:\n",
        "\n",
        "Precision: 99.826%</br>\n",
        "Recall: 99.826%</br>\n",
        "F1-score: 99.826%</br>\n",
        "Accuracy: 99.826%</br>\n",
        "\n",
        "Detailed Analysis:</br>\n",
        "\n",
        "The assessment of the results and the confusion matrix reveals that the model had only one misclassification error. This exceptional performance indicates the high quality of the model.\n",
        "\n",
        "However, it is crucial to perform cross-validation to confirm the model's robustness and to rule out the possibility of overfitting. For more information about the metrics visit [12.8 FastText + SVM](https://github.com/Inteli-College/2024-1B-T10-SI06-G01/blob/main/documentos/documentation.md#128-fasttext-svm). \n",
        "\n",
        "\n",
        "\n",
        "**Explanation of Overfitting:**\n",
        "\n",
        "Overfitting occurs when an algorithm excessively or even precisely adapts to the training data, resulting in a model that cannot make accurate predictions or conclusions with data other than the training set. Such a characteristic leads to high metric results. To prevent overfitting, cross-validation is necessary, as represented below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUd9ORw-MXje"
      },
      "source": [
        "### Cross Validation of FastText + SVM\n",
        "\n",
        "Cross-validation is a technique used to evaluate the performance of a machine learning model. Instead of simply splitting the dataset into a training set and a separate test set, cross-validation involves dividing the dataset into multiple subsets, or \"folds\". The model is trained on several combinations of these folds, and the performance metrics are averaged across these folds to provide a more robust estimate of the model's performance. This helps to ensure that the model's performance is not heavily influenced by the particular choice of training and test sets. In this code, we'll be using cross-validation to find the best parameters for our SVM classifier, which will help improve its generalization performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bArZoWlhMV-e"
      },
      "outputs": [],
      "source": [
        "# Download necessary nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr2JrDEGMgy3"
      },
      "source": [
        "Defines a function to preprocess text by converting to lowercase, removing punctuation and numbers, tokenizing, and removing stop words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Loi4yPJ8MhvZ"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Text preprocessing function:\n",
        "    - Converts text to lowercase\n",
        "    - Removes punctuation\n",
        "    - Removes numbers\n",
        "    - Tokenizes the text\n",
        "    - Removes stop words\n",
        "    - Joins words back into a string\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to preprocess\n",
        "\n",
        "    Returns:\n",
        "    str: The preprocessed text\n",
        "    \"\"\"\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    words = word_tokenize(text)  # Tokenization\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
        "    text = ' '.join(words)  # Join words back into a string\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHl0h-oRMjLh"
      },
      "source": [
        "Loads the dataset, assuming it has columns named 'comment' and 'sentiment', and stores the data in lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aeilw45WMkF0"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "#dataset = pd.read_csv(\"classification-labeled.csv\", sep=\";\")\n",
        "\n",
        "# Assuming the dataset has columns 'comment' and 'sentiment'\n",
        "texts = dataset['comment'].tolist()\n",
        "labels = dataset['sentiment'].tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqNvxE90Mlzb"
      },
      "source": [
        "Preprocesses the text data and converts labels to the format expected by FastText.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8XIrU-EMnCs"
      },
      "outputs": [],
      "source": [
        "# Preprocess the texts\n",
        "texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "# Convert labels to FastText format (__label__<label>)\n",
        "labels = [f'__label__{label}' for label in labels]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXk-qwKdMoP7"
      },
      "source": [
        "Creates a temporary training data file in the format expected by FastText.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WNO-BTlMpwT"
      },
      "outputs": [],
      "source": [
        "# Create a training data file for FastText\n",
        "train_data_path = 'train_data.txt'\n",
        "with open(train_data_path, 'w', encoding='utf-8') as f:\n",
        "    for text, label in zip(texts, labels):\n",
        "        f.write(f'{label} {text}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Au2TIllMq0x"
      },
      "source": [
        "Trains a FastText model using hierarchical softmax and saves the trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N78FgPKMrwN"
      },
      "outputs": [],
      "source": [
        "# Train FastText model using hierarchical softmax (hs)\n",
        "model = fasttext.train_supervised(input=train_data_path, lr=1.0, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='hs')\n",
        "\n",
        "# Save the FastText model\n",
        "model.save_model('fasttext_model_hs.bin')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM30MSgtMuPf"
      },
      "source": [
        "Defines a function to convert text to vectors using the trained FastText model and converts all texts to vectors. Labels are converted back to integers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paXB-BZCMseG"
      },
      "outputs": [],
      "source": [
        "def text_to_vector(text):\n",
        "    \"\"\"\n",
        "    Converts text to vector using the trained FastText model\n",
        "\n",
        "    Args:\n",
        "    text (str): The text to convert\n",
        "\n",
        "    Returns:\n",
        "    np.ndarray: The vector representation of the text\n",
        "    \"\"\"\n",
        "    return model.get_sentence_vector(text)\n",
        "\n",
        "# Convert texts to vectors\n",
        "X = [text_to_vector(text) for text in texts]\n",
        "\n",
        "# Convert labels back to integers\n",
        "y = [int(label.replace('__label__', '')) for label in labels]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGm7CgRpMv5P"
      },
      "source": [
        "Splits the data into training and testing sets and applies SMOTE to the training set to handle class imbalance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUA1Lsp_MxKE"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE to the training set\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPHnP7LcMzc3"
      },
      "source": [
        "Defines the parameter grid for GridSearchCV, initializes the SVM classifier, and performs GridSearchCV to find the best parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkpfTnmJM0jb"
      },
      "outputs": [],
      "source": [
        "# Define parameters for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'degree': [2, 3, 4],  # for 'poly' kernel\n",
        "    'gamma': ['scale', 'auto']  # for 'rbf', 'poly', 'sigmoid' kernels\n",
        "}\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svm_classifier = svm.SVC()\n",
        "\n",
        "# Configure GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Display the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23uYWqXHM2Fn"
      },
      "source": [
        "Trains the SVM classifier with the best parameters, makes predictions on the test data, and evaluates the performance using accuracy, precision, recall, and F1-score metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pKIOL3zM3Gf"
      },
      "outputs": [],
      "source": [
        "# Train SVM with the best parameters\n",
        "best_svm_classifier = grid_search.best_estimator_\n",
        "best_svm_classifier.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_test = best_svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate performance on test data\n",
        "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
        "precision_test = precision_score(y_test, y_pred_test, average='weighted')\n",
        "recall_test = recall_score(y_test, y_pred_test, average='weighted')\n",
        "f1_score_test = fbeta_score(y_test, y_pred_test, beta=2, average='weighted')\n",
        "\n",
        "print(\"\\nPerformance metrics on test data:\")\n",
        "print(f'Accuracy: {accuracy_test}')\n",
        "print(f'Precision: {precision_test}')\n",
        "print(f'Recall: {recall_test}')\n",
        "print(f'F1-score: {f1_score_test}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuwA1yc8M35d"
      },
      "source": [
        "Defines a function to plot learning curves and plots the learning curves for the best SVM classifier. This helps to visualize the model's performance with different training sizes and cross-validation scores.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr7MHYqPM5A2"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Plots learning curves for a given estimator\n",
        "\n",
        "    Args:\n",
        "    estimator: The model/estimator to evaluate\n",
        "    title (str): Title of the plot\n",
        "    X (np.ndarray): Feature data\n",
        "    y (np.ndarray): Target labels\n",
        "    ylim (tuple): Tuple defining the limits for the y-axis\n",
        "    cv: Cross-validation splitting strategy\n",
        "    n_jobs: Number of jobs to run in parallel\n",
        "    train_sizes (np.ndarray): Array of training sizes\n",
        "\n",
        "    Returns:\n",
        "    plt: The plot\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training Examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "# Plot learning curves for the SVM\n",
        "title = \"Learning Curves (SVM)\"\n",
        "plot_learning_curve(best_svm_classifier, title, X, y, cv=5, n_jobs=-1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8um69cZ0YIHz"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j45fXjw3YOYF"
      },
      "source": [
        "### Dependencies Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29GmjZm6YaPa"
      },
      "source": [
        "Unfortunetly, transformers require a specific version of tensorflow which is compatible with tf_hub and tf_text. This requires us to reimport and reinstall some of the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tpJVOKbwYJ0E",
        "outputId": "aeb857ee-7a31-45d2-f914-ade2267abc52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting protobuf==3.20\n",
            "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n",
            "Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.48.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.24.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow 2.8.4 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.20.0 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.8.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "542fc6a692b04b77a0d03d4cc8d15ba5",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.15\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (24.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15) (1.63.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.0\n",
            "    Uninstalling protobuf-3.20.0:\n",
            "      Successfully uninstalled protobuf-3.20.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.4\n",
            "    Uninstalling tensorflow-2.8.4:\n",
            "      Successfully uninstalled tensorflow-2.8.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\n",
            "tensorflow-text 2.8.1 requires tensorflow<2.9,>=2.8.0, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 protobuf-4.25.3 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow_hub==0.12.0 in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub==0.12.0) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub==0.12.0) (4.25.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: tensorflow_text==2.8.1 in /usr/local/lib/python3.10/dist-packages (2.8.1)\n",
            "Collecting tensorflow<2.9,>=2.8.0 (from tensorflow_text==2.8.1)\n",
            "  Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text==2.8.1) (0.12.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (24.3.25)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.3.0)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-estimator<2.9,>=2.8 (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1)\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1)\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.63.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1)\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1)\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.1) (3.2.2)\n",
            "Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl (498.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-estimator, keras, tensorboard-data-server, protobuf, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.7.2\n",
            "    Uninstalling tensorboard-data-server-0.7.2:\n",
            "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.3\n",
            "    Uninstalling protobuf-4.25.3:\n",
            "      Successfully uninstalled protobuf-4.25.3\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.8.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 protobuf-3.19.6 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorflow-2.8.4 tensorflow-estimator-2.8.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "05098c454d074743833553970b292206",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install protobuf==3.20\n",
        "!pip install tensorflow==2.15\n",
        "!pip install tensorflow_hub==0.12.0\n",
        "!pip install tensorflow_text==2.8.1\n",
        "##precisa clicar em cancel quando o collab tenta fazer restartar o runtime NÃO PODE RESTARTAR O COLLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lEV4g5MWYvb3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from tensorflow.keras.models import Sequential\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input,Dense,Dropout\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras import Model\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import roc_auc_score, f1_score, roc_curve, confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ng3TfNY4zf"
      },
      "source": [
        "### Data preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "q1LBH2_wY7mV",
        "outputId": "0c047d52-c610-49a3-ce41-395f28eb45b2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 370,\n        \"min\": 1395,\n        \"max\": 2397,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2005,\n          2397,\n          1395\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\\"macron corruptly helping uber accrue profit\\\" was not on my bingo cards\",\n          \"Uber deliberately dodged authorities, ignored rules in early years, leaked documents show | CBC News  https://t.co/S97PrEGe8D\",\n          \" Hey Uber,  I took a trip on 5th July in Delhi which was in non cash mode but driver insisted on paying cash only. Even after paying whole amount in cash it is showing outstanding in my account and preventing me to take any further rides. Please sort it out.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": -1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3fef1f5f-cf09-4ebd-aaeb-a5fffd2ca3ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1662</th>\n",
              "      <td>1946</td>\n",
              "      <td>\"While French taxi drivers staged sometimes vi...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1721</th>\n",
              "      <td>2005</td>\n",
              "      <td>\"macron corruptly helping uber accrue profit\" ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>1395</td>\n",
              "      <td>Hey Uber,  I took a trip on 5th July in Delhi...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1870</th>\n",
              "      <td>2154</td>\n",
              "      <td>Shame on Uber! #uber #brokelaw   https://t.co/...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2113</th>\n",
              "      <td>2397</td>\n",
              "      <td>Uber deliberately dodged authorities, ignored ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3fef1f5f-cf09-4ebd-aaeb-a5fffd2ca3ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3fef1f5f-cf09-4ebd-aaeb-a5fffd2ca3ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3fef1f5f-cf09-4ebd-aaeb-a5fffd2ca3ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-79465fee-9e2e-45b4-b84d-1601dca78f12\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-79465fee-9e2e-45b4-b84d-1601dca78f12')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-79465fee-9e2e-45b4-b84d-1601dca78f12 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        id                                            comment  sentiment\n",
              "1662  1946  \"While French taxi drivers staged sometimes vi...         -1\n",
              "1721  2005  \"macron corruptly helping uber accrue profit\" ...         -1\n",
              "1116  1395   Hey Uber,  I took a trip on 5th July in Delhi...         -1\n",
              "1870  2154  Shame on Uber! #uber #brokelaw   https://t.co/...         -1\n",
              "2113  2397  Uber deliberately dodged authorities, ignored ...         -1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "#df = pd.read_csv(\"src/data/classification-labeled.csv\", encoding='ISO-8859-1', delimiter=';')\n",
        "df = pd.read_csv('/content/drive/MyDrive/dbs/classification-labeled.csv', encoding='ISO-8859-1', delimiter=';')\n",
        "\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdUvIOWCY_SY",
        "outputId": "c4d52362-ecd5-410e-b08f-6734ec75b2e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylxd1SHyZDKE"
      },
      "source": [
        "#### Class Balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3egmW1WdZPXw"
      },
      "source": [
        "Here, after testing with infered data from SMOTE, the ideal solution for the class balance seem to be undersampling, one technique of undersampling that is used is joining both the 0 (neutral) and 1 (positive) labels in only one (0). Since this generates a better overall precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UkPq3XQEZPhz"
      },
      "outputs": [],
      "source": [
        "df['sentiment'] = df['sentiment'].replace({1: 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-ajdngEZjew",
        "outputId": "c62f696d-b032-4179-eeac-6f7e1b741d39"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "0    866\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_positive = df[df['sentiment']==0 ]\n",
        "df_positive.shape\n",
        "df_positive['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8RlqYUqZkaH",
        "outputId": "31ac4d0e-2648-41f2-acee-88e4288d2368"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2010, 3)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_negative = df[df['sentiment']== -1 ]\n",
        "df_negative.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dJQdriVZljf",
        "outputId": "73dfb2c9-9fb4-4c9c-917a-3c95a073ea1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sentiment\n",
              "-1    2010\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_negative['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL1l6Kx1Znqr"
      },
      "source": [
        "Undersampling the 2010 negative comments to 866 comments. Then the classes will have the same number of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vup3ojmSZn2O",
        "outputId": "81a2c952-24fd-41d8-eef7-0d270fe36f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1732, 3)\n",
            "sentiment\n",
            "-1    866\n",
            " 0    866\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df_positive = df[df['sentiment'] == 0]\n",
        "df_negative = df[df['sentiment'] == -1]\n",
        "\n",
        "df_negative_downsample = df_negative.sample(n=df_positive.shape[0], random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_negative_downsample, df_positive])\n",
        "\n",
        "print(df_balanced.shape)\n",
        "print(df_balanced['sentiment'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "87Yyp8u2ZqIa"
      },
      "outputs": [],
      "source": [
        "df = df_balanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxOdFlslZpE_"
      },
      "source": [
        "Encoding the classes for -1 (negative) to be 0 and 0 (positives and neutrals) to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "odq7cgbAZrq2"
      },
      "outputs": [],
      "source": [
        "df['sentiment'] = df['sentiment'].map({-1: 0, 0: 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af3D7kt5ZwDj"
      },
      "source": [
        "Basic preprocessing, filtering and tokenizing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8CZj5As_Zv2B",
        "outputId": "ce1fe956-4efc-408e-81c6-b22ad13eb536"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 663,\n        \"min\": 627,\n        \"max\": 2026,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1641,\n          627,\n          2026\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"? feel like Uber today lol\",\n          \"$ UBER Uber broke laws , duped police secretly lobbied governments , leak reveals https : //t.co/tybg3bdaYB\",\n          \"Uber broke laws , duped police built secret lobbying operation , leak reveals https : //t.co/x5D72POw3F\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-82de7720-1298-4719-b02d-2c43d1ce45ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>647</td>\n",
              "      <td>Uber , railroad conductors .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>1641</td>\n",
              "      <td>? feel like Uber today lol</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1742</th>\n",
              "      <td>2026</td>\n",
              "      <td>Uber broke laws , duped police built secret lo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>673</td>\n",
              "      <td>. employers simply pay workers instead relying...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>627</td>\n",
              "      <td>$ UBER Uber broke laws , duped police secretly...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82de7720-1298-4719-b02d-2c43d1ce45ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-82de7720-1298-4719-b02d-2c43d1ce45ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-82de7720-1298-4719-b02d-2c43d1ce45ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5b793fb0-5a35-4479-8a2a-b473b54fafa8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b793fb0-5a35-4479-8a2a-b473b54fafa8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5b793fb0-5a35-4479-8a2a-b473b54fafa8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        id                                            comment  sentiment\n",
              "377    647                       Uber , railroad conductors .          1\n",
              "1361  1641                         ? feel like Uber today lol          1\n",
              "1742  2026  Uber broke laws , duped police built secret lo...          0\n",
              "403    673  . employers simply pay workers instead relying...          1\n",
              "372    627  $ UBER Uber broke laws , duped police secretly...          0"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "df['comment'] = df['comment'].apply(remove_stopwords)\n",
        "\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmQE9R4fZynY",
        "outputId": "c1f35720-4bd8-4e3e-8db5-7d56f5f31c1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['sentiment'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W25ntrOCZ0DH"
      },
      "source": [
        "Splitting the dataframe in train and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SeT5r4ehZ0MA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"comment\"], df[\"sentiment\"], test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFrMf3vZZ4A-"
      },
      "source": [
        "### Inicialization of BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu4z-9YBZ5z0"
      },
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) is a natural language processing model based on the Transformer architecture. It is trained to understand the context of words in texts through a method called masked language modeling, where some words are hidden and the model tries to predict them. Additionally, it also learns the relationship between sentence pairs. BERT's ability to analyze texts bidirectionally, considering the full context of each word, makes it efficient for a variety of NLP tasks, such as sentiment analysis!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b-n-poMZ7Ho"
      },
      "source": [
        "It is also important to list and understand how BERT works:\n",
        "\n",
        "1. Input Representation:\n",
        "- Tokenization: BERT starts by tokenizing the text into tokens that can be found in its vocabulary. It uses WordPiece tokenization, splitting words into subwords or tokens. For example, \"smiling\" might be split into \"smile\" and \"##ing\".\n",
        "- Special Tokens: It adds special tokens like [CLS] at the beginning of the text and [SEP] to separate segments or denote the end of a sentence.\n",
        "Segment Embeddings: If there are two sentences (like in question-answering tasks), BERT can differentiate them using segment embeddings A and B.\n",
        "- Positional Embeddings: Since the Transformer architecture does not inherently capture sequence order, BERT adds positional embeddings to give the model information about the position of tokens in the sequence.\n",
        "\n",
        "2. Processing by Transformer Blocks:\n",
        "- Layers: BERT processes the input sequence through multiple layers (12 for BERT-base, 24 for BERT-large) of Transformer blocks.\n",
        "- Self-Attention: Each Transformer block uses self-attention mechanisms that allow the model to weigh the importance of all tokens in the input sequence, regardless of their position. This means each token is influenced by all other tokens.\n",
        "- Bidirectionality: This self-attention mechanism is what allows BERT to be bidirectional, meaning it learns information from both the left and right context of a token simultaneously.\n",
        "- Feed-Forward Neural Networks: Each layer also contains feed-forward neural networks that transform the representation at each position of the sequence.\n",
        "\n",
        "3. Output\n",
        "- Contextualized Embeddings: The output from the final Transformer layer is a set of contextualized word embeddings. Unlike traditional embeddings, these represent not just the word, but also the context in which it appears.\n",
        "- Task-Specific Layers: For specific tasks, these embeddings are passed through additional layers. For instance, for classification tasks, the output corresponding to the [CLS] token is typically used to make predictions. For question answering, the model predicts the start and end positions of the answer in the context.\n",
        "\n",
        "4. Pre-training and Fine-tuning\n",
        "- Pre-training: Before being used for specific tasks, BERT is pre-trained on a large corpus of text. It learns by predicting 15% of the words in each sequence, which have been randomly masked (Masked Language Model), and by predicting whether a second sentence in a pair is the subsequent sentence in the original text (Next Sentence Prediction).\n",
        "- Fine-tuning: After pre-training, BERT can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks like classification, entity recognition, and question answering, with relatively minimal task-specific data.\n",
        "\n",
        "For more information about the model visit documentation  [session 9.6 Transformers - BERT](https://github.com/Inteli-College/2024-1B-T10-SI06-G01/blob/main/documentos/documentation.md#96-transformers-bert). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9hDAU-WaFax"
      },
      "source": [
        "Here the preprocessor and encoder are imported and started from BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7uE6BMiuZ4qp"
      },
      "outputs": [],
      "source": [
        "url_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "\n",
        "url_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/2\"\n",
        "\n",
        "bert_preprocess = hub.KerasLayer(url_preprocess)\n",
        "bert_encoder = hub.KerasLayer(url_encoder, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "u49MzfB0aHi4"
      },
      "outputs": [],
      "source": [
        "text_input = Input(shape=(), dtype=tf.string,name='Comment')\n",
        "Preprocessor = bert_preprocess(text_input)\n",
        "output = bert_encoder(Preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAkpCxAVaI8u"
      },
      "source": [
        "The layers of the neural network are defined, such as the activation functions, the dropout rates and sets the model as trainable, which means that it can shift the weight of different atributes at it is beeing used\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jjT3nzSAaJGY"
      },
      "outputs": [],
      "source": [
        "x = Dense(256, activation='relu', name=\"Dense_256\")(output[\"pooled_output\"])\n",
        "x = Dropout(0.5, name=\"dropout_1\")(x)\n",
        "x = Dense(64, activation='relu', name=\"Dense_64\")(x)\n",
        "x = Dropout(0.5, name=\"dropout_2\")(x)\n",
        "outputs = Dense(1, activation='sigmoid', name=\"output\")(x)\n",
        "\n",
        "model = Model(inputs=[text_input], outputs=outputs)\n",
        "\n",
        "model.trainable = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umhRZ5xYaKe6",
        "outputId": "afd945b3-2d79-4c96-a6f0-9bcda41d186f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Comment (InputLayer)           [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       {'input_mask': (Non  0           ['Comment[0][0]']                \n",
            "                                e, 128),                                                          \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " keras_layer_1 (KerasLayer)     {'default': (None,   6368641     ['keras_layer[0][0]',            \n",
            "                                128),                             'keras_layer[0][1]',            \n",
            "                                 'pooled_output': (               'keras_layer[0][2]']            \n",
            "                                None, 128),                                                       \n",
            "                                 'sequence_output':                                               \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 'encoder_outputs':                                               \n",
            "                                 [(None, 128, 128),                                               \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128),                                                \n",
            "                                 (None, 128, 128)]}                                               \n",
            "                                                                                                  \n",
            " Dense_256 (Dense)              (None, 256)          33024       ['keras_layer_1[0][13]']         \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 256)          0           ['Dense_256[0][0]']              \n",
            "                                                                                                  \n",
            " Dense_64 (Dense)               (None, 64)           16448       ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64)           0           ['Dense_64[0][0]']               \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1)            65          ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,418,178\n",
            "Trainable params: 6,418,177\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YoXDTpzaNZn",
        "outputId": "c0ed95a9-a834-4b03-c395-04991fb45f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:absl:hub.KerasLayer is trainable but has zero trainable weights.\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7a8116d48e80>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7a8116d48e80>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "38/38 [==============================] - 129s 3s/step - loss: 0.7000 - accuracy: 0.5429 - val_loss: 0.5959 - val_accuracy: 0.7000\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 99s 3s/step - loss: 0.6051 - accuracy: 0.6947 - val_loss: 0.5010 - val_accuracy: 0.7673\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 100s 3s/step - loss: 0.5387 - accuracy: 0.7384 - val_loss: 0.4649 - val_accuracy: 0.7885\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 98s 3s/step - loss: 0.4755 - accuracy: 0.7904 - val_loss: 0.4811 - val_accuracy: 0.7750\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 99s 3s/step - loss: 0.4496 - accuracy: 0.8053 - val_loss: 0.4335 - val_accuracy: 0.7923\n",
            "Epoch 6/10\n",
            " 4/38 [==>...........................] - ETA: 1:16 - loss: 0.5067 - accuracy: 0.7500"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=Adam(learning_rate=5e-5),\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ6t54bnaTST"
      },
      "source": [
        "The confusion matrix is plotted so we can check the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwOPtP1yaQ2f"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "conf_mat = confusion_matrix(y_test, y_pred_binary)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more information about the metrics visit [12.4 Transformers](https://github.com/Inteli-College/2024-1B-T10-SI06-G01/blob/main/documentos/documentation.md#124-transformers). \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
